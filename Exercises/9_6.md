# Exercise 9.6

Consider a special case of a Gaussian mixture model in which the covariance matrices $\Sigma_k$ of the components are all constrained to have a common value $\Sigma$. Derive the EM equations for maximizing the likelihood function under such a model.

## Solution

This is a bit ambiguously formulated, what it means is we have
$$
\Sigma = \Sigma_1 = \Sigma_2 = \dots = \Sigma_K
$$
and so our parameters are
$$
\theta = (\pi_1, \dots, \pi_K, \mu_1, \dots, \mu_K, \Sigma).
$$

Note that the $E$ steps stays the same (just with $\Sigma$ instead of $\Sigma_k$):
$$
\gamma_{nk} = \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma)}{\sum{j = 1}^K \pi_j \mathcal{N}(x_n|\mu_j, \Sigma) }
$$

Our likelihood on the complete data is
$$
p(X, Z|\theta) = \prod_{n = 1}^N \prod_{k = 1}^K \pi_k^{z_{nk}} \mathcal{N}(x_n|\mu_k, \Sigma)^{z_{nk}}.
$$

We can apply the logarithm to obtain
$$
\log(p(X, Z|\theta)) = \sum_{n = 1}^N \sum_{k = 1}^K z_{nk} \left(\log(\pi_k) + \log(\mathcal{N}(x_n|\mu_k, \Sigma)) \right)
$$
It can be computed 
$$
\begin{aligned}
&\sum_{n = 1}^N \sum_{k = 1}^K z_{nk} \log(\mathcal{N}(x_n|\mu_k, \Sigma)) \\
=& \sum_{n = 1}^N \sum_{k = 1}^K z_{nk} \left(-\frac{1}{2}\log\left((2\pi)^D \det(\Sigma)\right) - \frac{1}{2}(x_n - \mu_k)\Sigma^{-1}(x_n - \mu_k)\right) \\
=& -\frac{1}{2} \sum_{n = 1}^N \sum_{k = 1}^K z_{nk}\left(D\log(2\pi) + \log(\det(\Sigma)) + (x_n - \mu_k)^\top\Sigma^{-1}(x_n - \mu_k)\right)
\end{aligned}
$$

Putting everything together yields
$$
Q(\theta, \theta^\text{old}) = \sum_{n = 1}^N \sum_{k = 1}^K q(\theta, \theta^\text{old}, n, k)
$$
where
$$
q(\theta, \theta^\text{old}, n, k) = \gamma_{nk} \left(\log(\pi_k) - \frac{1}{2} C - \frac{1}{2} \log(\det(\Sigma)) - \frac{1}{2}(x_n - \mu_k)^\top \Sigma^{-1}(x_n - \mu_k)\right)
$$
where $C := D \log(2\pi)$ and
$$
\gamma_{nk} = \frac{\pi_k^\text{old} \mathcal{N}(x_n|\mu_k^\text{old}, \Sigma^\text{old})}{\sum_{j = 1}^K \pi_j^\text{old} \mathcal{N}(x_n|\mu_j^\text{old}, \Sigma^\text{old}) }
$$
is treated as a constant.

Now want to optimise with respect to the $\pi_k, \mu_k$ and $\Sigma$. The derivations for the $\pi_k$ and $\mu_k$ are the same as in the normal MoG case:

We optimise
$$
\begin{aligned}
Q'(\pi_1, \dots, \pi_K, \lambda) &= \sum_{n = 1}^N \sum_{k = 1}^K \gamma_{nk}  \log(\pi_k) + \lambda \left(\sum_{k = 1}^K \pi_k - 1 \right) + \text{const} \\
&= \sum_{k = 1}^K N_k \log(\pi_k) + \lambda\left(\sum_{k = 1}^K \pi_k - 1 \right) + \text{const}
\end{aligned}
$$
using Lagrange multipliers and get
$$
\pi_k = \frac{N_k}{N}.
$$
We optimise 
$$
Q''(\mu_1, \dots, \mu_k) = \sum_{n = 1}^N \sum_{k = 1}^K \gamma_{nk} (x_n - \mu_k)^\top \Sigma^{-1}(x_n - \mu_k) + \text{const},
$$
to obtain
$$
\mu_k = \frac{1}{N_k}\sum_{n = 1}^N \gamma_{nk} x_n.
$$

The only new part is optimising this expression:
$$
\begin{aligned}
Q'''(\Sigma) &= -\frac{1}{2} \sum_{n = 1}^N \sum_{k = 1}^K \gamma_{nk} \log(\det(\Sigma)) + \gamma_{nk} (x_n - \mu_k)^\top \Sigma^{-1}(x_n - \mu_k) + \text{const} \\
&= -\frac{1}{2} N \log(\det(\Sigma)) + \sum_{n = 1}^N \sum_{k = 1}^K \gamma_{nk} (x_n - \mu_k)^\top \Sigma^{-1}(x_n - \mu_k) + \text{const}
\end{aligned}
$$

I won't go through the algebra to compute this, but the resulting $\Sigma$ is
$$
\Sigma = \frac{1}{N} \sum_{n = 1}^N \sum_{k = 1}^K \gamma_{nk}(x_n - \mu_k)(x_n - \mu_k)^\top.
$$

Recall that for the normal case we have
$$
\Sigma_k = \frac{1}{N_k} \sum_{k = 1}^N \gamma_{nk}(x_n - \mu_k)(x_n - \mu_k)^\top,
$$
in this shared covariance case
$$
\Sigma = \sum_{k = 1}^K \Sigma_k,
$$
where the $\Sigma_k$ are what we would get in the normal MoG M step.