# Exercise 9.4

Suppose we wish to use the EM algorithm to maximize the posterior distribution over parameters $p(\theta|X)$ for a model containing latent variables, where $X$ is the observed data set. Show that the E step remains the same as in the maximum likelihood case, whereas in the M step the quantity to be maximized is given by $\mathcal{Q}(\theta, \theta^\text{old}) + \log(p(\theta))$ where $\mathcal{Q}(\theta, \theta^\text{old})$ is deÔ¨Åned by (9.30).

## Solution

E Step obviously stays the same as that only concerns the posterior of the latent variable and not what function we average over.

Recall that
$$
p(X|Y) = P(Y|X) \frac{P(X)}{P(Y)}.\tag{1}
$$
by Bayes.

In the EM algo we have
$$
\begin{aligned}
Q(\theta, \theta^\text{old}) &= \mathbb{E}_{z \sim p(Z|X, \theta^\text{old})}[\log(p(X, Z|\theta))] \\
&= \sum_Z \log(p(X, Z| \theta))p(Z|X, \theta^\text{old}).\tag{2}
\end{aligned}
$$
For this exercise we are interested in computing
$$
\begin{aligned}
Q'(\theta, \theta^\text{old}) &= \mathbb{E}_{Z \sim p(Z|X, \theta^\text{old})}[\log(p(\theta, Z | X))] \\
&= \sum_{Z} \log(p(\theta, Z | X)) p(Z|X, \theta^\text{old}) \\
&\overset{1}{=} \sum_Z \log\left(\frac{p(X, Z|\theta)p(\theta)}{p(X)}\right) p(Z|X, \theta^\text{old}) \\
&= \sum_Z \log(p(X, Z|\theta))p(Z|X, \theta^\text{old}) + \sum_Z \log(p(\theta)) p(Z|X, \theta^\text{old}) - \sum_Z \log(p(X)) p(Z|X, \theta^\text{old}) \\
&\overset{2}{=} Q(\theta, \theta^\text{old}) + \mathbb{E}_{Z \sim p(Z|X, \theta^\text{old})}(\log(p(\theta))) - \mathbb{E}_{Z \sim p(Z|X, \theta^\text{old})}[\log(p(X))] \\
&\overset{3}{=} Q(\theta, \theta^\text{old}) + \log(p(\theta)) - \log(p(X))
\end{aligned}
$$
where we have used in (3) the fact that $\log(p(\theta))$ and $\log(p(X))$ are independent of $Z$ so their expectation is just the value.

Because $\log(p(X))$ is independent of $\theta$ we can drop it for the maximisation and obtain
$$
Q(\theta, \theta^\text{old}) + \log(p(\theta))
$$
as new optimisation target for the M Step.