# Exercise 5.1
Consider a two-layer network function of the form (5.7) in which the hidden-unit nonlinear activation functions $ g(\cdot) $ are given by logistic sigmoid functions of the form

$$
\sigma(a) = \{1 + \exp(-a)\}^{-1}. \tag{5.191}
$$

Show that there exists an equivalent network, which computes exactly the same function, but with hidden unit activation functions given by $ \tanh(a) $, where the $ \tanh $ function is defined by (5.59). **Hint**: first find the relation between $ \sigma(a) $ and $ \tanh(a) $, and then show that the parameters of the two networks differ by linear transformations.

$$
y_k(\mathbf{x}, \mathbf{w}) = \sigma\left( \sum_{j=1}^M w_{kj}^{(2)} h\left( \sum_{i=1}^D w_{ji}^{(1)} x_i + w_{j0}^{(1)} \right) + w_{k0}^{(2)} \right). \tag{5.7}
$$

$$
\tanh(a) = \frac{e^a - e^{-a}}{e^a + e^{-a}}. \tag{5.59}
$$

## Solution
It is easy to verify that
$$
\tanh(x) = 2 \sigma(2x) - 1 \tag{$*$}
$$
with that the analoguous function to (5.59) is given by
$$
\tilde{y}_k(\underline{x}, \underline{w}) = \tanh\left( \sum_{j=1}^M w_{kj}^{(2)} h\left( \sum_{i=1}^D w_{ji}^{(1)} x_i + w_{j0}^{(1)} \right) + w_{k0}^{(2)} \right) \tag{$**$}
$$

We can write
$$
\underline{w} = \begin{pmatrix} \underline{w}^{(1)} \\ \underline{w}^{(2)} \end{pmatrix}.
$$
Applying $(*)$ to $(**)$ we obtian
$$
\begin{aligned}
\tilde{y}_k\left(\underline{x}, \underline{w} \right) &= 2 \sigma\left( 2 \left[ \sum_{j=1}^M w_{kj}^{(2)} h\left( \sum_{i=1}^D w_{ji}^{(1)} x_i + w_{j0}^{(1)} \right) + w_{k0}^{(2)} \right]\right) - 1 \\
&= 2 \sigma\left( \left[ \sum_{j=1}^M \left(2w_{kj}^{(2)}\right) h\left( \sum_{i=1}^D w_{ji}^{(1)} x_i + w_{j0}^{(1)} \right) + \left(2w_{k0}^{(2)}\right) \right]\right) - 1  \\
&= 2 y_k\left(\underline{x}, \begin{pmatrix} \underline{w}^{(1)} \\ 2\underline{w}^{(2)} \end{pmatrix}\right) - 1.
\end{aligned}
$$
As such $\tilde{y}_k$ and $y_k$ differ up to a linear function applied to the weights and an affine linear function applied to the output.